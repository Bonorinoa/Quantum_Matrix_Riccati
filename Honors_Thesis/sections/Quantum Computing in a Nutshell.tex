\chapter{Quantum Computing in a Nutshell}
\section{Historical Background}
I believe that reviewing the challenges and actors that played a key role in the development of Quantum Computing is necessary for a comprehensive understanding of this nascent technology's true potential. In the following paragraphs I will attempt to illustrate the birth of Quantum Computing, in a nutshell. 

The first fundamental breakthrough in the development of Quantum Mechanics came from the related area of Chemistry during the early 19th century. John Dalton's atomic theory, which states that elements are composed by identical and indivisible particles we know call atoms \cite{daltons_2014}. Dalton's discovery opened the door for curious physicists of that era to ponder about the nature of these atoms and their role in our universe. One hundred years passed until the nascent theory of quantum mechanics experienced another breakthrough of such magnitude. In the early 1900s, German theoretical physicist Max Planck observed an extraneous pattern in the information outputted by his measurement tools. From this subtle observation Planck developed his famous Planck's law, which postulates that the electromagnetic energy is emitted not continuously, unlike classical physics which assumes this to be true, but by discrete portions or quants \cite{khairoutdinov_plancks_2012}.

Danish physicist Nielhs Bohr used Planck's work to gain a deeper understanding of atoms and their behavior. Along with his protégés, such as Enrico Fermi and Werner Heisenberg, Bohr lead a revolutionary movement within the world of Physics powered by mind-blowing scientific discoveries \cite{loeffler_niels_2018}. The concepts of superposition, entanglement and the wave-particle duality model were developed. Quantum Mechanics was born. Although novel, most physicists of the time considered their ideas ridiculous or incomprehensible. Among the most famous critics of Bohr's work we find Albert Einstein, who failed to make sense of the probabilistic universe quantum mechanics was proposing. Multiple spirited discussions were exchanged between them until Einstein, Podolsky and Rosen proved their famous EPR paradox \cite{epr_paradox}, which supported the entanglement theory that Bohr's was proposing.

\section{Feynman's vision and the Quantum Turing Machine (QTM)}

By the 1950s Quantum Physics was a well-established sub-discipline of Physics that focuses in understanding the characteristics and behavior of atoms. Efforts proved very successful. Among the plethora of advancements credited to quantum mechanics there is the manipulation of chemical reactions, ideas that fueled the development of the atomic bomb during the 1940s. But, the Manhattan Project left us with something else besides an atomic bomb and a deeper understanding of quantum objects: Richard Feynman. With less than 30 years of age, Dr. Feynman was in charge of running the computing unit responsible for making the calculations behind the first atomic bombs \cite{selwood_richard_2018}. After receiving the Nobel Prize in Physics in 1965, alongside Sin-Itiro Tomonaga and Julian Schwinger, "for their fundamental work in quantum electrodynamics, with deep-ploughing consequences for the physics of elementary particles." \cite{nobel_physics} Dr. Feynman embarked on a new challenge: Simulating quantum phenomena. 

Curious about the limitations of classical computers governed by the laws of Physics and the possibility of simulating Nature, Richard Feynman proposed in 1980 a radical computer architecture based on the laws of quantum mechanics \cite{feynman_simulating_1981}. The problem was that the world we inhabit is inherent probabilistic, yet computers were deterministic by definition. Thus, we are left with two logical options: Design a probabilistic computer or figure out a way to design a classical universal automaton capable of this kind of simulations. 

This novel architecture, now called Quantum Computing, exploits ideas of quantum physics like superposition, linear association amongst qubits' states, and entanglement, a peculiar correlation between two particles (i.e. qubits) regardless of how far apart they are from each other, to process high dimensional computations more efficiently. For instance, consider the task of folding a protein. A supercomputer would try to check every single combination the protein could adopt, an approach that grows exponentially in complexity as the size of the protein increases. In contrast, a Quantum Algorithm proceeds by creating multidimensional spaces where the patterns linking individual data points emerge \cite{ibm_quantum}. Nevertheless, current quantum computers, called Noisy-Intermediate-Scale-Quantum (NISQ) computers, are very sensitive to changes in the environment, like heat fluctuations. Therefore, we are not able to retrieve a desired result with certainty. Understanding how to reduce the uncertainty, or interference, of quantum algorithms is being actively studied by the field of Quantum Error Correction \cite{somoroff_quantum_2018}.

Recently, there has been a growing focus on hybrid architectures. For instance, in March of 2022 researchers from Google Quantum AI proposed a framework to combine quantum algorithms with classical monte carlo simulation to study chemistry \cite{huggins_hybrid_2022}. These are algorithms composed of both a classical and a quantum component. By taking advantage of quantum hardware, we can process computationally expensive subroutines of a classical algorithm with a smaller, noisy, quantum algorithm more efficiently. Hence, the overall performance of the architecture increases. In this thesis I propose a novel hybrid architecture for approximating solutions to systems of linear equations, that combines an existing quantum algorithm with classical procedures, and apply it to solving a Matrix Riccati equation.